HF_HUB_OFFLINE=1 python scripts/benchmark.py --method streaming_llm --max_tokens 2024 --num_samples 10
======================================================================
KV Cache Compression Benchmark
======================================================================

Configuration:
  Model: EleutherAI/pythia-70m-deduped
  Method: streaming_llm
  Skip layers: [0, 1]
  Number of samples: 10
  Max eval tokens: 2024
  Max new tokens: 500
Loading model: EleutherAI/pythia-70m-deduped
Using device: mps

Loading PG-19 dataset...
  Found local file: /Users/od/Desktop/NLP/CS3602-LLM-Inference-Acceleration/data/pg19.parquet
  Loaded 100 samples from local file
  Sample 1: 249431 characters
  Sample 2: 318194 characters
  Sample 3: 309465 characters
  Sample 4: 343094 characters
  Sample 5: 23870 characters
  Sample 6: 216914 characters
  Sample 7: 432117 characters
  Sample 8: 128528 characters
  Sample 9: 345713 characters
  Sample 10: 100250 characters
  Total: 10 samples loaded

Methods to test:
  - baseline: {}
  - streaming_256: {'start_size': 4, 'recent_size': 252}
  - streaming_512: {'start_size': 4, 'recent_size': 508}
  - streaming_1024: {'start_size': 4, 'recent_size': 1020}

================================================================================
AGGREGATED RESULTS (averaged across samples)
================================================================================

Method                       TTFT(s)    TPOT(s)    Thruput        PPL        Acc    Cache
------------------------------------------------------------------------------------------
baseline                      0.0090     0.0059     168.71      39.99     35.49%     2023
streaming_256                 0.0065     0.0074     135.37      43.06     34.49%     2023
streaming_512                 0.0074     0.0071     140.05      41.45     35.04%     2023
streaming_1024                0.0076     0.0074     136.03      40.78     35.31%     2023
==========================================================================================

Comparison with baseline:
  streaming_256: TTFT +27.5%, PPL +7.7%, Acc -2.8%
  streaming_512: TTFT +17.5%, PPL +3.7%, Acc -1.3%
  streaming_1024: TTFT +16.1%, PPL +2.0%, Acc -0.5%

====================================================
运行命令行：
python scripts/benchmark.py --method streaming_llm \
--max_tokens 5000 \
--recent_sizes 252,508,1020 \
--num_samples 5

================================================================================
AGGREGATED RESULTS (averaged across samples)
================================================================================

Method                       TTFT(s)    TPOT(s)    Thruput        PPL        Acc    Cache
------------------------------------------------------------------------------------------
baseline                      0.0163     0.0083     123.30     124.28     26.19%     4999
streaming_256                 0.0131     0.0115      90.36     168.93     25.10%     4999
streaming_512                 0.0138     0.0106      94.40     164.10     25.46%     4999
streaming_1024                0.0140     0.0110      90.58     161.96     25.66%     4999
==========================================================================================

Comparison with baseline:
  streaming_256: TTFT +19.8%, PPL +35.9%, Acc -4.2%
  streaming_512: TTFT +15.3%, PPL +32.0%, Acc -2.8%
  streaming_1024: TTFT +14.2%, PPL +30.3%, Acc -2.0%

==================================================================
运行命令行：
python scripts/benchmark.py --method streaming_llm \
    --max_tokens 50000 \
    --recent_sizes 252,508,1020 \
    --num_samples 3


